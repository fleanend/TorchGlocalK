{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fleanend/TorchGlocalK/blob/main/Glocal_K.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ty3gYQgtnwFA"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/usydnlp/Glocal_K.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nl2tU6kL8Ot3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "379fe638-9ba3-4513-c9d0-6bbd279e03e2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fbecdbac430>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from time import time\n",
        "from scipy.sparse import csc_matrix\n",
        "import numpy as np\n",
        "import h5py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "torch.manual_seed(1284)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4A9uU1WloQ2"
      },
      "source": [
        "# Data Loader Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cq3KEUaVo1o3"
      },
      "outputs": [],
      "source": [
        "def load_data_100k(path='./', delimiter='\\t'):\n",
        "\n",
        "    train = np.loadtxt(path+'movielens_100k_u1.base', skiprows=0, delimiter=delimiter).astype('int32')\n",
        "    test = np.loadtxt(path+'movielens_100k_u1.test', skiprows=0, delimiter=delimiter).astype('int32')\n",
        "    total = np.concatenate((train, test), axis=0)\n",
        "\n",
        "    n_u = np.unique(total[:,0]).size  # num of users\n",
        "    n_m = np.unique(total[:,1]).size  # num of movies\n",
        "    n_train = train.shape[0]  # num of training ratings\n",
        "    n_test = test.shape[0]  # num of test ratings\n",
        "\n",
        "    train_r = np.zeros((n_m, n_u), dtype='float32')\n",
        "    test_r = np.zeros((n_m, n_u), dtype='float32')\n",
        "\n",
        "    for i in range(n_train):\n",
        "        train_r[train[i,1]-1, train[i,0]-1] = train[i,2]\n",
        "\n",
        "    for i in range(n_test):\n",
        "        test_r[test[i,1]-1, test[i,0]-1] = test[i,2]\n",
        "\n",
        "    train_m = np.greater(train_r, 1e-12).astype('float32')  # masks indicating non-zero entries\n",
        "    test_m = np.greater(test_r, 1e-12).astype('float32')\n",
        "\n",
        "    print('data matrix loaded')\n",
        "    print('num of users: {}'.format(n_u))\n",
        "    print('num of movies: {}'.format(n_m))\n",
        "    print('num of training ratings: {}'.format(n_train))\n",
        "    print('num of test ratings: {}'.format(n_test))\n",
        "\n",
        "    return n_m, n_u, train_r, train_m, test_r, test_m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_8kEkg9mlIW"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0fkA1WpmipzF"
      },
      "outputs": [],
      "source": [
        "# Insert the path of a data directory by yourself (e.g., '/content/.../data')\n",
        "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
        "data_path = ''\n",
        "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJqSSY33mgkw",
        "outputId": "54f7ca43-b9f7-4edb-8628-783a4513f4f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data matrix loaded\n",
            "num of users: 943\n",
            "num of movies: 1682\n",
            "num of training ratings: 80000\n",
            "num of test ratings: 20000\n"
          ]
        }
      ],
      "source": [
        "# Data Load\n",
        "try:\n",
        "    path = data_path + '/content/Glocal_K/data/MovieLens_100K/'\n",
        "    n_m, n_u, train_r, train_m, test_r, test_m = load_data_100k(path=path, delimiter='\\t')\n",
        "except Exception:\n",
        "    print('Error: Unable to load data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nGCdp_FlobOK"
      },
      "outputs": [],
      "source": [
        "# Common hyperparameter settings\n",
        "n_hid = 500 # size of hidden layers\n",
        "n_dim = 5 # inner AE embedding size\n",
        "n_layers = 2 # number of hidden layers\n",
        "gk_size = 3 # width=height of kernel for convolution\n",
        "\n",
        "# Hyperparameters to tune for specific case\n",
        "max_epoch_p = 500 # max number of epochs for pretraining\n",
        "max_epoch_f = 1000 # max number of epochs for finetuning\n",
        "patience_p = 5 # number of consecutive rounds of early stopping condition before actual stop for pretraining\n",
        "patience_f = 10 # and finetuning\n",
        "tol_p = 1e-4 # minimum threshold for the difference between consecutive values of train rmse, used for early stopping, for pretraining\n",
        "tol_f = 1e-5 # and finetuning\n",
        "lambda_2 = 20. # regularisation of number or parameters\n",
        "lambda_s = 0.006 # regularisation of sparsity of the final matrix\n",
        "dot_scale = 1 # dot product weight for global kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sWtU4-pmDDT"
      },
      "source": [
        "# Network Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "p1P6fgYiy28F"
      },
      "outputs": [],
      "source": [
        "def local_kernel(u, v):\n",
        "    dist = torch.norm(u - v, p=2, dim=2)\n",
        "    hat = torch.clamp(1. - dist**2, min=0.)\n",
        "    return hat\n",
        "\n",
        "class KernelLayer(nn.Module):\n",
        "    def __init__(self, n_in, n_hid, n_dim, lambda_s, lambda_2, activation=nn.Sigmoid()):\n",
        "      super().__init__()\n",
        "      self.W = nn.Parameter(torch.randn(n_in, n_hid))\n",
        "      self.u = nn.Parameter(torch.randn(n_in, 1, n_dim))\n",
        "      self.v = nn.Parameter(torch.randn(1, n_hid, n_dim))\n",
        "      self.b = nn.Parameter(torch.randn(n_hid))\n",
        "\n",
        "      self.lambda_s = lambda_s\n",
        "      self.lambda_2 = lambda_2\n",
        "\n",
        "      nn.init.xavier_uniform_(self.W, gain=torch.nn.init.calculate_gain(\"relu\"))\n",
        "      nn.init.xavier_uniform_(self.u, gain=torch.nn.init.calculate_gain(\"relu\"))\n",
        "      nn.init.xavier_uniform_(self.v, gain=torch.nn.init.calculate_gain(\"relu\"))\n",
        "      nn.init.zeros_(self.b)\n",
        "      self.activation = activation\n",
        "\n",
        "    def forward(self, x):\n",
        "      w_hat = local_kernel(self.u, self.v)\n",
        "    \n",
        "      sparse_reg = torch.nn.functional.mse_loss(w_hat, torch.zeros_like(w_hat))\n",
        "      sparse_reg_term = self.lambda_s * sparse_reg\n",
        "      \n",
        "      l2_reg = torch.nn.functional.mse_loss(self.W, torch.zeros_like(self.W))\n",
        "      l2_reg_term = self.lambda_2 * l2_reg\n",
        "\n",
        "      W_eff = self.W * w_hat  # Local kernelised weight matrix\n",
        "      y = torch.matmul(x, W_eff) + self.b\n",
        "      y = self.activation(y)\n",
        "\n",
        "      return y, sparse_reg_term + l2_reg_term\n",
        "\n",
        "class KernelNet(nn.Module):\n",
        "    def __init__(self, n_u, n_hid, n_dim, n_layers, lambda_s, lambda_2):\n",
        "      super().__init__()\n",
        "      layers = []\n",
        "      for i in range(n_layers):\n",
        "        if i == 0:\n",
        "          layers.append(KernelLayer(n_u, n_hid, n_dim, lambda_s, lambda_2))\n",
        "        else:\n",
        "          layers.append(KernelLayer(n_hid, n_hid, n_dim, lambda_s, lambda_2))\n",
        "      layers.append(KernelLayer(n_hid, n_u, n_dim, lambda_s, lambda_2, activation=nn.Identity()))\n",
        "      self.layers = nn.ModuleList(layers)\n",
        "      self.dropout = nn.Dropout(0.33)\n",
        "\n",
        "    def forward(self, x):\n",
        "      total_reg = None\n",
        "      for i, layer in enumerate(self.layers):\n",
        "        x, reg = layer(x)\n",
        "        if i < len(self.layers)-1:\n",
        "          x = self.dropout(x)\n",
        "        if total_reg is None:\n",
        "          total_reg = reg\n",
        "        else:\n",
        "          total_reg += reg\n",
        "      return x, total_reg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7RGKh1ckXgtP"
      },
      "outputs": [],
      "source": [
        "class CompleteNet(nn.Module):\n",
        "    def __init__(self, kernel_net, n_u, n_m, n_hid, n_dim, n_layers, lambda_s, lambda_2, gk_size, dot_scale):\n",
        "      super().__init__()\n",
        "      self.gk_size = gk_size\n",
        "      self.dot_scale = dot_scale\n",
        "      self.local_kernel_net = kernel_net\n",
        "      self.conv_kernel = torch.nn.Parameter(torch.randn(n_m, gk_size**2) * 0.1)\n",
        "      nn.init.xavier_uniform_(self.conv_kernel, gain=torch.nn.init.calculate_gain(\"relu\"))\n",
        "      \n",
        "\n",
        "    def forward(self, x, x_local):\n",
        "      gk = self.global_kernel(x_local, self.gk_size, self.dot_scale)\n",
        "      x = self.global_conv(x, gk)\n",
        "      x, global_reg_loss = self.local_kernel_net(x)\n",
        "      return x, global_reg_loss\n",
        "\n",
        "    def global_kernel(self, input, gk_size, dot_scale):\n",
        "      avg_pooling = torch.mean(input, dim=1)  # Item (axis=1) based average pooling\n",
        "      avg_pooling = avg_pooling.view(1, -1)\n",
        "\n",
        "      gk = torch.matmul(avg_pooling, self.conv_kernel) * dot_scale  # Scaled dot product\n",
        "      gk = gk.view(1, 1, gk_size, gk_size)\n",
        "\n",
        "      return gk\n",
        "\n",
        "    def global_conv(self, input, W):\n",
        "      input = input.unsqueeze(0).unsqueeze(0)\n",
        "      conv2d = nn.LeakyReLU()(F.conv2d(input, W, stride=1, padding=1))\n",
        "      return conv2d.squeeze(0).squeeze(0)\n",
        "\n",
        "class Loss(nn.Module):\n",
        "    def forward(self, pred_p, reg_loss, train_m, train_r):\n",
        "      # L2 loss\n",
        "      diff = train_m * (train_r - pred_p)\n",
        "      sqE = torch.nn.functional.mse_loss(diff, torch.zeros_like(diff))\n",
        "      loss_p = sqE + reg_loss\n",
        "      return loss_p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8sQCwrSmKG4"
      },
      "source": [
        "# Network Instantiation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOtWj1SCo1RW"
      },
      "source": [
        "## Pre-training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7teUrgWagpW0"
      },
      "outputs": [],
      "source": [
        "model = KernelNet(n_u, n_hid, n_dim, n_layers, lambda_s, lambda_2).double().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IEBsNhNo4Cj"
      },
      "source": [
        "## Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OiTXqnN6zLXQ"
      },
      "outputs": [],
      "source": [
        "complete_model = CompleteNet(model, n_u, n_m, n_hid, n_dim, n_layers, lambda_s, lambda_2, gk_size, dot_scale).double().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sETwz58aK6y6"
      },
      "source": [
        "# Evaluation code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vyReXxgac3KH"
      },
      "outputs": [],
      "source": [
        "def dcg_k(score_label, k):\n",
        "    dcg, i = 0., 0\n",
        "    for s in score_label:\n",
        "        if i < k:\n",
        "            dcg += (2**s[1]-1) / np.log2(2+i)\n",
        "            i += 1\n",
        "    return dcg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jwsSR-8ZdGWo"
      },
      "outputs": [],
      "source": [
        "def ndcg_k(y_hat, y, k):\n",
        "    score_label = np.stack([y_hat, y], axis=1).tolist()\n",
        "    score_label = sorted(score_label, key=lambda d:d[0], reverse=True)\n",
        "    score_label_ = sorted(score_label, key=lambda d:d[1], reverse=True)\n",
        "    norm, i = 0., 0\n",
        "    for s in score_label_:\n",
        "        if i < k:\n",
        "            norm += (2**s[1]-1) / np.log2(2+i)\n",
        "            i += 1\n",
        "    dcg = dcg_k(score_label, k)\n",
        "    return dcg / norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "yy9eQS51pbhj"
      },
      "outputs": [],
      "source": [
        "def call_ndcg(y_hat, y):\n",
        "    ndcg_sum, num = 0, 0\n",
        "    y_hat, y = y_hat.T, y.T\n",
        "    n_users = y.shape[0]\n",
        "\n",
        "    for i in range(n_users):\n",
        "        y_hat_i = y_hat[i][np.where(y[i])]\n",
        "        y_i = y[i][np.where(y[i])]\n",
        "\n",
        "        if y_i.shape[0] < 2:\n",
        "            continue\n",
        "\n",
        "        ndcg_sum += ndcg_k(y_hat_i, y_i, y_i.shape[0])  # user-wise calculation\n",
        "        num += 1\n",
        "\n",
        "    return ndcg_sum / num"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXXQjeMxmYEC"
      },
      "source": [
        "# Training and Test Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZ35Zoha-Eue",
        "outputId": "4fc0c647-b0a5-4e69-afff-899c14dc247d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 0 test rmse: 2.7594793 train rmse: 2.7418816\n",
            "Time: 1.8956103324890137 seconds\n",
            "Time cumulative: 1.8956103324890137 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 29 test rmse: 1.1400328 train rmse: 1.1055552\n",
            "Time: 38.90566849708557 seconds\n",
            "Time cumulative: 618.9266548156738 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n"
          ]
        }
      ],
      "source": [
        "best_rmse_ep, best_mae_ep, best_ndcg_ep = 0, 0, 0\n",
        "best_rmse, best_mae, best_ndcg = float(\"inf\"), float(\"inf\"), 0\n",
        "\n",
        "time_cumulative = 0\n",
        "tic = time()\n",
        "\n",
        "# Pre-Training\n",
        "optimizer = torch.optim.AdamW(complete_model.local_kernel_net.parameters(), lr=0.001)\n",
        "\n",
        "def closure():\n",
        "  optimizer.zero_grad()\n",
        "  x = torch.Tensor(train_r).double().to(device)\n",
        "  m = torch.Tensor(train_m).double().to(device)\n",
        "  complete_model.local_kernel_net.train()\n",
        "  pred, reg = complete_model.local_kernel_net(x)\n",
        "  loss = Loss().to(device)(pred, reg, m, x)\n",
        "  loss.backward()\n",
        "  return loss\n",
        "\n",
        "last_rmse = np.inf\n",
        "counter = 0\n",
        "\n",
        "for i in range(max_epoch_p):\n",
        "  optimizer.step(closure)\n",
        "  complete_model.local_kernel_net.eval()\n",
        "  t = time() - tic\n",
        "  time_cumulative += t\n",
        "\n",
        "  pre, _ = model(torch.Tensor(train_r).double().to(device))\n",
        "  \n",
        "  pre = pre.float().cpu().detach().numpy()\n",
        "  \n",
        "  error = (test_m * (np.clip(pre, 1., 5.) - test_r) ** 2).sum() / test_m.sum()  # test error\n",
        "  test_rmse = np.sqrt(error)\n",
        "\n",
        "  error_train = (train_m * (np.clip(pre, 1., 5.) - train_r) ** 2).sum() / train_m.sum()  # train error\n",
        "  train_rmse = np.sqrt(error_train)\n",
        "\n",
        "  if last_rmse-train_rmse < tol_p:\n",
        "    counter += 1\n",
        "  else:\n",
        "    counter = 0\n",
        "\n",
        "  last_rmse = train_rmse\n",
        "\n",
        "  if patience_p == counter:\n",
        "    print('.-^-._' * 12)\n",
        "    print('PRE-TRAINING')\n",
        "    print('Epoch:', i+1, 'test rmse:', test_rmse, 'train rmse:', train_rmse)\n",
        "    print('Time:', t, 'seconds')\n",
        "    print('Time cumulative:', time_cumulative, 'seconds')\n",
        "    print('.-^-._' * 12)\n",
        "    break\n",
        "\n",
        "\n",
        "  if i % 50 != 0:\n",
        "    continue\n",
        "  print('.-^-._' * 12)\n",
        "  print('PRE-TRAINING')\n",
        "  print('Epoch:', i, 'test rmse:', test_rmse, 'train rmse:', train_rmse)\n",
        "  print('Time:', t, 'seconds')\n",
        "  print('Time cumulative:', time_cumulative, 'seconds')\n",
        "  print('.-^-._' * 12)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-Tuning\n",
        "\n",
        "train_r_local = np.clip(pre, 1., 5.)\n",
        "\n",
        "optimizer = torch.optim.AdamW(complete_model.parameters(), lr=0.001)\n",
        "\n",
        "def closure():\n",
        "  optimizer.zero_grad()\n",
        "  x = torch.Tensor(train_r).double().to(device)\n",
        "  x_local = torch.Tensor(train_r_local).double().to(device)\n",
        "  m = torch.Tensor(train_m).double().to(device)\n",
        "  complete_model.train()\n",
        "  pred, reg = complete_model(x, x_local)\n",
        "  loss = Loss().to(device)(pred, reg, m, x)\n",
        "  loss.backward()\n",
        "  return loss\n",
        "\n",
        "last_rmse = np.inf\n",
        "counter = 0\n",
        "\n",
        "for i in range(max_epoch_f):\n",
        "  optimizer.step(closure)\n",
        "  complete_model.eval()\n",
        "  t = time() - tic\n",
        "  time_cumulative += t\n",
        "\n",
        "  pre, _ = complete_model(torch.Tensor(train_r).double().to(device), torch.Tensor(train_r_local).double().to(device))\n",
        "  \n",
        "  pre = pre.float().cpu().detach().numpy()\n",
        "\n",
        "  error = (test_m * (np.clip(pre, 1., 5.) - test_r) ** 2).sum() / test_m.sum()  # test error\n",
        "  test_rmse = np.sqrt(error)\n",
        "\n",
        "  error_train = (train_m * (np.clip(pre, 1., 5.) - train_r) ** 2).sum() / train_m.sum()  # train error\n",
        "  train_rmse = np.sqrt(error_train)\n",
        "\n",
        "  test_mae = (test_m * np.abs(np.clip(pre, 1., 5.) - test_r)).sum() / test_m.sum()\n",
        "  train_mae = (train_m * np.abs(np.clip(pre, 1., 5.) - train_r)).sum() / train_m.sum()\n",
        "\n",
        "  test_ndcg = call_ndcg(np.clip(pre, 1., 5.), test_r)\n",
        "  train_ndcg = call_ndcg(np.clip(pre, 1., 5.), train_r)\n",
        "\n",
        "  if test_rmse < best_rmse:\n",
        "      best_rmse = test_rmse\n",
        "      best_rmse_ep = i+1\n",
        "\n",
        "  if test_mae < best_mae:\n",
        "      best_mae = test_mae\n",
        "      best_mae_ep = i+1\n",
        "\n",
        "  if best_ndcg < test_ndcg:\n",
        "      best_ndcg = test_ndcg\n",
        "      best_ndcg_ep = i+1\n",
        "\n",
        "  if last_rmse-train_rmse < tol_f:\n",
        "    counter += 1\n",
        "  else:\n",
        "    counter = 0\n",
        "\n",
        "  last_rmse = train_rmse\n",
        "\n",
        "  if patience_f == counter:\n",
        "    print('.-^-._' * 12)\n",
        "    print('FINE-TUNING')\n",
        "    print('Epoch:', i+1, 'test rmse:', test_rmse, 'test mae:', test_mae, 'test ndcg:', test_ndcg)\n",
        "    print('Epoch:', i+1, 'train rmse:', train_rmse, 'train mae:', train_mae, 'train ndcg:', train_ndcg)\n",
        "    print('Time:', t, 'seconds')\n",
        "    print('Time cumulative:', time_cumulative, 'seconds')\n",
        "    print('.-^-._' * 12)\n",
        "    break\n",
        "\n",
        "\n",
        "  if i % 50 != 0:\n",
        "    continue\n",
        "\n",
        "  print('.-^-._' * 12)\n",
        "  print('FINE-TUNING')\n",
        "  print('Epoch:', i, 'test rmse:', test_rmse, 'test mae:', test_mae, 'test ndcg:', test_ndcg)\n",
        "  print('Epoch:', i, 'train rmse:', train_rmse, 'train mae:', train_mae, 'train ndcg:', train_ndcg)\n",
        "  print('Time:', t, 'seconds')\n",
        "  print('Time cumulative:', time_cumulative, 'seconds')\n",
        "  print('.-^-._' * 12)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6v_tODcweLn",
        "outputId": "5ab3f058-5c7c-458a-c3b5-32bbcf4df418"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 0 test rmse: 1.0700573 test mae: 0.85894084 test ndcg: 0.8431631026304213\n",
            "Epoch: 0 train rmse: 1.0270153 train mae: 0.8224455 train ndcg: 0.8510749748058567\n",
            "Time: 40.45642638206482 seconds\n",
            "Time cumulative: 659.3830811977386 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 50 test rmse: 0.93911487 test mae: 0.7419493 test ndcg: 0.8893964614072852\n",
            "Epoch: 50 train rmse: 0.87197775 train mae: 0.6892665 train ndcg: 0.9032817944305084\n",
            "Time: 139.79444479942322 seconds\n",
            "Time cumulative: 5229.4451632499695 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 100 test rmse: 0.9173736 test mae: 0.7238552 test ndcg: 0.8959945381023721\n",
            "Epoch: 100 train rmse: 0.8513352 train mae: 0.67233855 train ndcg: 0.9073981492989845\n",
            "Time: 240.34458231925964 seconds\n",
            "Time cumulative: 14804.157754421234 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 150 test rmse: 0.91210085 test mae: 0.7186104 test ndcg: 0.8983912449433552\n",
            "Epoch: 150 train rmse: 0.85025233 train mae: 0.6705124 train ndcg: 0.9081461238508642\n",
            "Time: 339.89231538772583 seconds\n",
            "Time cumulative: 29342.110981464386 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 200 test rmse: 0.91075677 test mae: 0.717713 test ndcg: 0.8999463422728763\n",
            "Epoch: 200 train rmse: 0.847425 train mae: 0.6684982 train ndcg: 0.9086662178729511\n",
            "Time: 437.7295274734497 seconds\n",
            "Time cumulative: 48828.542159080505 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 250 test rmse: 0.9089356 test mae: 0.7162371 test ndcg: 0.9004734993425247\n",
            "Epoch: 250 train rmse: 0.8429641 train mae: 0.66512245 train ndcg: 0.9095270439566602\n",
            "Time: 537.3215084075928 seconds\n",
            "Time cumulative: 73261.3013408184 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 300 test rmse: 0.9095531 test mae: 0.7178777 test ndcg: 0.9002791914048373\n",
            "Epoch: 300 train rmse: 0.84128505 train mae: 0.6646636 train ndcg: 0.9111313869875\n",
            "Time: 635.322491645813 seconds\n",
            "Time cumulative: 102619.57430267334 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 350 test rmse: 0.9094079 test mae: 0.7184808 test ndcg: 0.8993963925102507\n",
            "Epoch: 350 train rmse: 0.84042495 train mae: 0.66467035 train ndcg: 0.9117860612263418\n",
            "Time: 734.9211509227753 seconds\n",
            "Time cumulative: 136903.35994291306 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 400 test rmse: 0.909184 test mae: 0.7176108 test ndcg: 0.8999755804453298\n",
            "Epoch: 400 train rmse: 0.8389591 train mae: 0.6627469 train ndcg: 0.911755660607807\n",
            "Time: 832.2777988910675 seconds\n",
            "Time cumulative: 176127.0729689598 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 450 test rmse: 0.90896446 test mae: 0.71646327 test ndcg: 0.8989898809899146\n",
            "Epoch: 450 train rmse: 0.8386111 train mae: 0.6618805 train ndcg: 0.9121938074699816\n",
            "Time: 930.8952333927155 seconds\n",
            "Time cumulative: 220248.3406779766 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 500 test rmse: 0.90928406 test mae: 0.71763897 test ndcg: 0.8992084651393832\n",
            "Epoch: 500 train rmse: 0.83785033 train mae: 0.662405 train ndcg: 0.9121748478131005\n",
            "Time: 1029.4480030536652 seconds\n",
            "Time cumulative: 269308.02171349525 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 550 test rmse: 0.9084498 test mae: 0.7172481 test ndcg: 0.8987156582079183\n",
            "Epoch: 550 train rmse: 0.8369074 train mae: 0.66179717 train ndcg: 0.9130644955664211\n",
            "Time: 1128.2734668254852 seconds\n",
            "Time cumulative: 323287.5112416744 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 600 test rmse: 0.90858674 test mae: 0.71653205 test ndcg: 0.8988671574285699\n",
            "Epoch: 600 train rmse: 0.83592033 train mae: 0.66057956 train ndcg: 0.9129902156549652\n",
            "Time: 1226.1112308502197 seconds\n",
            "Time cumulative: 382193.1678235531 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 650 test rmse: 0.9076746 test mae: 0.71587485 test ndcg: 0.8983348925954459\n",
            "Epoch: 650 train rmse: 0.83532226 train mae: 0.65924966 train ndcg: 0.9134813013852621\n",
            "Time: 1326.1585321426392 seconds\n",
            "Time cumulative: 446065.63185071945 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 700 test rmse: 0.9083985 test mae: 0.71558785 test ndcg: 0.8992407902202504\n",
            "Epoch: 700 train rmse: 0.83459526 train mae: 0.6583956 train ndcg: 0.9137079416972551\n",
            "Time: 1423.878006696701 seconds\n",
            "Time cumulative: 514860.74871468544 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 750 test rmse: 0.9078382 test mae: 0.7159553 test ndcg: 0.8985292679256924\n",
            "Epoch: 750 train rmse: 0.8337771 train mae: 0.6582501 train ndcg: 0.9141852957954192\n",
            "Time: 1524.3319008350372 seconds\n",
            "Time cumulative: 588592.5313222408 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 800 test rmse: 0.90800387 test mae: 0.71561205 test ndcg: 0.8987147827639267\n",
            "Epoch: 800 train rmse: 0.8331633 train mae: 0.65751755 train ndcg: 0.9141623090426534\n",
            "Time: 1622.0958354473114 seconds\n",
            "Time cumulative: 667294.2881822586 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 850 test rmse: 0.9086336 test mae: 0.71650654 test ndcg: 0.8988400694967618\n",
            "Epoch: 850 train rmse: 0.8326722 train mae: 0.65762913 train ndcg: 0.9146795773552026\n",
            "Time: 1720.8921868801117 seconds\n",
            "Time cumulative: 750901.7676284313 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 900 test rmse: 0.9079395 test mae: 0.7152888 test ndcg: 0.8997024639702064\n",
            "Epoch: 900 train rmse: 0.8314887 train mae: 0.65604097 train ndcg: 0.9150456793320765\n",
            "Time: 1819.5796897411346 seconds\n",
            "Time cumulative: 839455.7945063114 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 950 test rmse: 0.90816706 test mae: 0.715623 test ndcg: 0.899260819654681\n",
            "Epoch: 950 train rmse: 0.83047175 train mae: 0.6552296 train ndcg: 0.914972006981081\n",
            "Time: 1917.079059123993 seconds\n",
            "Time cumulative: 932927.0447552204 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTi_PdXJqTjh",
        "outputId": "6f5a2d63-2c1f-446f-8f20-d4357d4bdc99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 773  best rmse: 0.9072191\n",
            "Epoch: 978  best mae: 0.71358\n",
            "Epoch: 308  best ndcg: 0.901110394527823\n"
          ]
        }
      ],
      "source": [
        "# Final result\n",
        "print('Epoch:', best_rmse_ep, ' best rmse:', best_rmse)\n",
        "print('Epoch:', best_mae_ep, ' best mae:', best_mae)\n",
        "print('Epoch:', best_ndcg_ep, ' best ndcg:', best_ndcg)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b6Yfh3hm4Efa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}